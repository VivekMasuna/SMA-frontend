import{j as e}from"./index-Ct3gel5r.js";/* empty css               */function r(){return e.jsxs("section",{className:"theory-container",children:[e.jsx("h1",{className:"theory-title",children:"Data Acquisition"}),e.jsxs("div",{className:"section",children:[e.jsx("h2",{children:"What is Data Acquisition?"}),e.jsx("p",{children:"Data acquisition is the process of collecting raw data from various sources and converting it into a structured format suitable for analysis. This step lays the foundation for any data-driven project, including those in machine learning, sentiment analysis, and big data analytics."})]}),e.jsxs("div",{className:"section",children:[e.jsx("h2",{children:"Sources of Data Acquisition"}),e.jsxs("ul",{className:"list-disc",children:[e.jsxs("li",{children:[e.jsx("strong",{children:"Web Scraping:"})," Extracting data directly from web pages using HTML parsers like Beautiful Soup or frameworks like Scrapy."]}),e.jsxs("li",{children:[e.jsx("strong",{children:"APIs:"})," Structured interfaces provided by platforms (e.g., Twitter, Reddit) to programmatically fetch real-time or historical data."]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Files:"})," Reading data stored in local files such as CSV, Excel, or JSON formats."]}),e.jsxs("li",{children:[e.jsx("strong",{children:"Databases:"})," Extracting data from SQL (e.g., MySQL, PostgreSQL) or NoSQL (e.g., MongoDB) databases."]})]})]}),e.jsxs("div",{className:"section",children:[e.jsx("h2",{children:"Tools & Techniques for Data Acquisition"}),e.jsxs("dl",{children:[e.jsx("dt",{children:"1. Beautiful Soup"}),e.jsxs("dd",{children:[e.jsx("b",{children:"Use Case:"})," Ideal for beginners and small‑scale web scraping."]}),e.jsxs("dd",{children:[e.jsx("b",{children:"Description:"})," A Python library used to parse HTML/XML documents."]}),e.jsx("dd",{children:e.jsx("b",{children:"Working: "})}),e.jsxs("ul",{className:"list-disc",children:[e.jsxs("li",{children:["Use the ",e.jsx("em",{children:"requests"})," library to fetch the web page."]}),e.jsx("li",{children:"Use BeautifulSoup to parse and extract the required data using tags, classes, or IDs."})]}),e.jsxs("dd",{children:[e.jsx("b",{children:"Pros:"})," Simple and easy to use; works well with static pages."]}),e.jsxs("dd",{children:[e.jsx("b",{children:"Limitations:"})," Not suitable for dynamic or JS‑rendered sites."]}),e.jsx("dt",{children:"2. Requests Library"}),e.jsxs("dd",{children:[e.jsx("b",{children:"Use Case:"})," Fetching web content or API responses."]}),e.jsxs("dd",{children:[e.jsx("b",{children:"Description:"})," Python library to send HTTP requests and receive content (HTML/JSON)."]}),e.jsx("dd",{children:e.jsx("b",{children:"Working:"})}),e.jsxs("ul",{className:"list-disc",children:[e.jsx("li",{children:"Send GET/POST requests to the target URL."}),e.jsx("li",{children:"Combine with BeautifulSoup or JSON parser to extract data."})]}),e.jsxs("dd",{children:[e.jsx("b",{children:"Pros:"})," Lightweight and straightforward."]}),e.jsxs("dd",{children:[e.jsx("b",{children:"Limitations:"})," Requires additional parsing tools."]}),e.jsx("dt",{children:"3. APIs"}),e.jsxs("dd",{children:[e.jsx("b",{children:"Use Case:"})," Preferred for structured, authorized data collection."]}),e.jsxs("dd",{children:[e.jsx("b",{children:"Description:"})," Official interfaces by platforms like Twitter, Reddit."]}),e.jsx("dd",{children:e.jsx("b",{children:"Working:"})}),e.jsxs("ul",{className:"list-disc",children:[e.jsx("li",{children:"Register and obtain API keys."}),e.jsx("li",{children:"Send HTTP requests with parameters and headers."}),e.jsx("li",{children:"Receive structured JSON/XML responses."})]}),e.jsxs("dd",{children:[e.jsx("b",{children:"Pros:"})," Reliable, legal, real‑time/historical data."]}),e.jsxs("dd",{children:[e.jsx("b",{children:"Limitations:"})," Rate limits, auth requirements, potential restrictions."]}),e.jsx("dt",{children:"4. Scrapy Tool"}),e.jsxs("dd",{children:[e.jsx("b",{children:"Use Case:"})," Best for large‑scale and complex scraping projects."]}),e.jsxs("dd",{children:[e.jsx("b",{children:"Description:"})," Powerful Python framework for web crawlers."]}),e.jsx("dd",{children:e.jsx("b",{children:"Working:"})}),e.jsxs("ul",{className:"list-disc",children:[e.jsx("li",{children:"Define spiders with custom crawling logic."}),e.jsx("li",{children:"Use CSS/XPath selectors to extract data."})]}),e.jsxs("dd",{children:[e.jsx("b",{children:"Pros:"})," Fast, scalable, built‑in exports."]}),e.jsxs("dd",{children:[e.jsx("b",{children:"Limitations:"})," Steeper learning curve and setup."]})]})]}),e.jsxs("div",{className:"section",children:[e.jsx("h2",{children:"Theory Summary"}),e.jsx("div",{className:"table-wrapper",children:e.jsxs("table",{className:"theory-table",children:[e.jsx("thead",{children:e.jsxs("tr",{children:[e.jsx("th",{children:"Tool/Technique"}),e.jsx("th",{children:"Best Use Case"}),e.jsx("th",{children:"Data Format"}),e.jsx("th",{children:"Skill Level"})]})}),e.jsxs("tbody",{children:[e.jsxs("tr",{children:[e.jsx("td",{children:"Beautiful Soup"}),e.jsx("td",{children:"Simple HTML scraping"}),e.jsx("td",{children:"HTML"}),e.jsx("td",{children:"Beginner"})]}),e.jsxs("tr",{children:[e.jsx("td",{children:"Requests"}),e.jsx("td",{children:"Fetching HTML/API content"}),e.jsx("td",{children:"HTML/JSON"}),e.jsx("td",{children:"Beginner"})]}),e.jsxs("tr",{children:[e.jsx("td",{children:"API"}),e.jsx("td",{children:"Real-time, structured, and reliable data"}),e.jsx("td",{children:"JSON/XML"}),e.jsx("td",{children:"Intermediate"})]}),e.jsxs("tr",{children:[e.jsx("td",{children:"Scrapy"}),e.jsx("td",{children:"Large-scale and fast web scraping"}),e.jsx("td",{children:"HTML"}),e.jsx("td",{children:"Advanced"})]})]})]})})]}),e.jsxs("div",{className:"section",children:[e.jsx("h2",{children:"Procedure"}),e.jsxs("ol",{className:"list-decimal",children:[e.jsx("li",{className:"procedure-step",children:"Choose Data Source: web page, API, or file."}),e.jsxs("li",{className:"procedure-step",children:["Collect Data:",e.jsxs("ul",{className:"list-disc",children:[e.jsx("li",{children:"Web scraping: requests + BeautifulSoup or Scrapy."}),e.jsx("li",{children:"API calls: requests, Postman, or Python clients (e.g., Tweepy)."}),e.jsx("li",{children:"Files: pandas, csv, or json libraries."})]})]}),e.jsx("li",{className:"procedure-step",children:"Clean & Preprocess: remove nulls/duplicates, handle inconsistencies."}),e.jsx("li",{className:"procedure-step",children:"Store Data: save cleaned data in CSV, JSON, or database."})]})]}),e.jsxs("div",{className:"section final-thoughts",children:[e.jsx("h2",{children:"Final Thoughts"}),e.jsxs("p",{children:["Beautiful Soup for simple, small‑scale tasks.",e.jsx("br",{}),"Requests as the core fetch tool.",e.jsx("br",{}),"APIs for structured, legal access.",e.jsx("br",{}),"Scrapy when you need speed & scale."]})]})]})}export{r as default};
